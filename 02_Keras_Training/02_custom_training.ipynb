{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "2.3.0\n"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 自动求导机制\n",
    "\n",
    "- **GradientTape**是eager模式下计算梯度用的\n",
    "\n",
    "上面的例子中的梯度计算部分可以更直观的理解这个函数的用法。\n",
    "```\n",
    "tf.GradientTape(\n",
    "    persistent=False,\n",
    "    watch_accessed_variables=True\n",
    ")\n",
    "```\n",
    "* `persistent` : 用来指定新创建的 `gradient tape`是否是可持续性的。默认是False，意味着只能够调用一次gradient()函数。\n",
    "* `watch_accessed_variables`: 表明这个`GradientTape`是不是会自动追踪任何能被训练（trainable）的变量。默认是True。要是为False的话，意味着你需要手动去指定你想追踪的那些变量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.constant(3.0)\n",
    "\n",
    "with tf.GradientTape() as g:\n",
    "    g.watch(x)\n",
    "    y = x * x\n",
    "    \n",
    "dy_dx = g.gradient(y, x)  # y’ = 2*x = 2*3 = 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "watch(tensor)\n",
    "```\n",
    "* 作用：确保某个`tensor` 被 `tape` 追踪 \n",
    "* 参数：`tensor` -> 一个 `Tensor` 或者一个 `Tensor` 列表\n",
    "\n",
    "```\n",
    "gradient(target, sources)\n",
    "```\n",
    "* 作用：根据tape上面的上下文来计算某个或者某些tensor的梯度参数\n",
    "\n",
    "      * target: 被微分的Tensor或者Tensor列表，你可以理解为经过某个函数之后的值\n",
    "      * sources: Tensors 或者Variables列表（当然可以只有一个值）. 你可以理解为函数的某个变量\n",
    "      \n",
    "* 返回:\n",
    "      一个列表表示各个变量的梯度值，和source中的变量列表一一对应，表明这个变量的梯度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tf.Tensor: shape=(), dtype=float32, numpy=6.0>"
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "dy_dx"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "构建模型（神经网络的前向传播）\n",
    "        ↓\n",
    "    定义损失函数\n",
    "        ↓\n",
    "    定义优化函数\n",
    "        ↓\n",
    "    定义tape\n",
    "        ↓\n",
    "    模型得到预测值\n",
    "        ↓\n",
    "    前向传播得到loss\n",
    "        ↓\n",
    "    反向传播\n",
    "        ↓\n",
    "用优化函数将计算出来的梯度更新到变量上面去"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 案例1、模型自动求导\n",
    "\n",
    "构建模型（神经网络的前向传播） -> 定义损失函数 -> 定义优化函数 -> 定义tape ->  模型得到预测值 -> 前向传播得到loss -> 反向传播 -> 用优化函数将计算出来的梯度更新到变量上面去"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(tf.keras.Model):\n",
    "\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(MyModel, self).__init__(name='my_model')\n",
    "        self.num_classes = num_classes\n",
    "        # 定义自己需要的层\n",
    "        self.dense_1 = tf.keras.layers.Dense(32, activation='relu')  # 隐藏层\n",
    "        self.dense_2 = tf.keras.layers.Dense(num_classes)  # 输出层\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # 定义前向传播\n",
    "        # 使用在 __init__() 定义的层\n",
    "        x = self.dense_1(inputs)\n",
    "        return self.dense_2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# 10分类问题\n",
    "data = np.random.random((1000, 32))\n",
    "labels = np.random.random((1000, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一般在网络中使用时，不需要显式调用watch函数 ，使用默认设置，GradientTape会监控可训练变量。\n",
    "```\n",
    "apply_gradients(grads_and_vars,name=None)\n",
    "```\n",
    "作用：把计算出来的梯度更新到变量上面去。\n",
    "参数含义 :\n",
    "* `grads_and_vars`：(gradient, variable) 对的列表 .\n",
    "* name: 操作名"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "WARNING:tensorflow:Layer my_model is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n\nIf you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n\nTo change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tf.Variable 'UnreadVariable' shape=() dtype=int64, numpy=1>"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "model = MyModel(num_classes=10)  # 构建前向传播模型\n",
    "\n",
    "loss_object = tf.keras.losses.CategoricalCrossentropy(from_logits=True)  # 定义损失函数\n",
    "optimizer = tf.keras.optimizers.Adam()  # 定义优化器\n",
    "\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    predictions = model(data)\n",
    "    loss = loss_object(labels, predictions)\n",
    "    \n",
    "gradients = tape.gradient(loss, model.trainable_variables)  # 求梯度\n",
    "\n",
    "optimizer.apply_gradients(zip(gradients, model.trainable_variables))  # 将梯度更新到每个值上"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[<tf.Variable 'my_model/dense/kernel:0' shape=(32, 32) dtype=float32, numpy=\n array([[ 0.2986059 , -0.22218348, -0.22293933, ...,  0.14016736,\n          0.01760756, -0.11621397],\n        [-0.05881743, -0.25921795,  0.2829372 , ..., -0.30361545,\n          0.18618977, -0.15834057],\n        [-0.2872937 , -0.14355828, -0.29375783, ..., -0.13330583,\n         -0.07523468,  0.2320581 ],\n        ...,\n        [-0.26255342,  0.08072431, -0.04701015, ..., -0.04800224,\n          0.26560968, -0.2825388 ],\n        [-0.11009346, -0.25503758,  0.2563971 , ...,  0.06066217,\n          0.13818401, -0.00197141],\n        [ 0.1448306 , -0.06286307, -0.02791359, ..., -0.17092887,\n         -0.24999507,  0.19709735]], dtype=float32)>,\n <tf.Variable 'my_model/dense/bias:0' shape=(32,) dtype=float32, numpy=\n array([-0.00099993, -0.00099947, -0.001     ,  0.0009918 ,  0.00099998,\n         0.001     ,  0.00099769, -0.001     ,  0.00100002, -0.00099975,\n         0.00100001,  0.00099961, -0.00099998, -0.001     , -0.00099967,\n         0.00100002,  0.00099996, -0.00099991, -0.00099998,  0.00099998,\n        -0.00099999,  0.00099981,  0.00099732, -0.00099018,  0.001     ,\n        -0.00099999, -0.00099386, -0.00099999,  0.00100002, -0.00099922,\n         0.00099999,  0.00100001], dtype=float32)>,\n <tf.Variable 'my_model/dense_1/kernel:0' shape=(32, 10) dtype=float32, numpy=\n array([[ 0.23213865, -0.23738259, -0.25395498,  0.14368474,  0.18037426,\n          0.07194889,  0.3159241 , -0.24433479, -0.22525364, -0.19380109],\n        [-0.04732239, -0.23749463, -0.30859104, -0.02820113, -0.32232428,\n         -0.2708836 ,  0.04585607, -0.17470466,  0.0852249 , -0.10722505],\n        [-0.25651243,  0.1808307 , -0.29991323, -0.0162545 ,  0.01464847,\n         -0.3017056 , -0.29944283,  0.16024128, -0.12605269,  0.20031229],\n        [ 0.17160808, -0.1567615 ,  0.08276775, -0.06452601,  0.02157179,\n         -0.05121597, -0.13279614,  0.16901247,  0.1358755 ,  0.2781531 ],\n        [ 0.1842534 ,  0.05216394, -0.15455693,  0.37571436,  0.20718776,\n         -0.23564659, -0.02883005, -0.12281481, -0.04197989,  0.17674647],\n        [-0.19356737,  0.02389441, -0.03932112,  0.17302164,  0.14246967,\n         -0.22814654,  0.3191276 ,  0.12262212,  0.16566983,  0.22858544],\n        [ 0.29223207, -0.13628127, -0.1435361 ,  0.00625908, -0.02306107,\n         -0.17849557, -0.01780131,  0.18254201,  0.16560054,  0.00508418],\n        [-0.31935132,  0.298802  , -0.06627738, -0.21469581, -0.14625064,\n         -0.31547686, -0.3036298 , -0.16493681,  0.18409736,  0.32277095],\n        [-0.01996503,  0.30018026,  0.15062857,  0.1973151 , -0.01475871,\n          0.17775029, -0.18532729,  0.33278918,  0.26619178, -0.12399388],\n        [ 0.2786584 , -0.24877286,  0.29250133, -0.04673269, -0.01275611,\n          0.08598521, -0.36451992,  0.28550398, -0.22593597, -0.06345195],\n        [-0.2510485 , -0.2881348 , -0.34923166,  0.33152157,  0.29469073,\n          0.22010267,  0.03328335,  0.03818509,  0.1591143 ,  0.3211586 ],\n        [ 0.1873923 , -0.02898002, -0.15515159,  0.31492314,  0.2311395 ,\n         -0.05603871,  0.31689   ,  0.28980312,  0.2916357 ,  0.25416455],\n        [ 0.0071526 , -0.0498382 ,  0.10765134,  0.36523655, -0.00191303,\n         -0.12670912, -0.21511576,  0.36700186, -0.36949384, -0.28207505],\n        [ 0.11707726, -0.17434242, -0.33587933,  0.21115917, -0.22603482,\n          0.03265914, -0.0781213 , -0.13833432,  0.2826671 ,  0.08843451],\n        [-0.13961233, -0.14512676,  0.24948359, -0.05091468,  0.18562374,\n         -0.06952142, -0.21992481, -0.09701704,  0.2741291 , -0.06908368],\n        [-0.23710223, -0.05208924, -0.12991697,  0.22730172,  0.2589422 ,\n          0.06879655,  0.25568348,  0.29202294,  0.177207  ,  0.17569625],\n        [-0.11852968, -0.26138574,  0.09201559,  0.15455176,  0.33684438,\n          0.08748062,  0.05529883,  0.19906665, -0.05712287, -0.24801531],\n        [-0.06828176,  0.10897408, -0.20061617,  0.2646717 , -0.18308379,\n         -0.00326886,  0.22566174,  0.08719421, -0.36245605, -0.19857606],\n        [-0.1266359 ,  0.3341597 , -0.17659694, -0.28473476, -0.07428911,\n         -0.20724419, -0.27980864,  0.05053211, -0.22776157, -0.29642236],\n        [ 0.24622858, -0.34009522, -0.11560266,  0.29862088,  0.07782938,\n          0.28306216,  0.00575132,  0.23771569,  0.27034298,  0.2384754 ],\n        [-0.1405868 , -0.02214083, -0.08911031, -0.22288309,  0.2229057 ,\n         -0.21464436,  0.16075557, -0.03460681, -0.13809767,  0.3440315 ],\n        [ 0.27973965,  0.3748602 ,  0.3282944 ,  0.2167798 ,  0.23683147,\n         -0.02872355,  0.05125319, -0.27519023,  0.167026  ,  0.07642303],\n        [ 0.30143684,  0.10660065, -0.07372335,  0.21315221, -0.10745607,\n         -0.24204524,  0.1745881 ,  0.03560665, -0.03983372,  0.36079264],\n        [ 0.3565741 ,  0.33605307,  0.04273487,  0.10940579,  0.08514577,\n         -0.21041508, -0.29004171,  0.08213745, -0.33160323, -0.07736194],\n        [ 0.05721643,  0.3229587 , -0.11100684, -0.24500483, -0.2420337 ,\n         -0.23162204,  0.35575813,  0.0735004 ,  0.13682441,  0.3652654 ],\n        [ 0.24528214, -0.2854224 , -0.05285776, -0.3551757 ,  0.29923615,\n         -0.06406773,  0.17240919, -0.02942539, -0.35146552,  0.22481938],\n        [ 0.00810359, -0.2072383 , -0.06056862, -0.13458663, -0.3446157 ,\n          0.31873453,  0.20116103, -0.07576958, -0.18031467,  0.28388342],\n        [-0.32147214,  0.01443209, -0.1847995 ,  0.14245982,  0.13786483,\n          0.14788961,  0.1500494 ,  0.06611178, -0.3755077 , -0.32731858],\n        [ 0.06266612, -0.24719262,  0.27752697,  0.14144444, -0.15412062,\n          0.2624004 ,  0.21052676, -0.21754359,  0.33738905, -0.07563064],\n        [-0.2999539 , -0.14671536,  0.11224975, -0.06944924,  0.01824976,\n          0.21541612,  0.33103207, -0.3089085 , -0.14922701,  0.3667591 ],\n        [ 0.3181889 ,  0.0801025 , -0.0537211 ,  0.13181823,  0.06396043,\n         -0.09129441, -0.01098746, -0.24642287, -0.15479521,  0.16041815],\n        [ 0.12126826,  0.37360263,  0.02539381, -0.28747356, -0.16444397,\n          0.23279518, -0.231021  ,  0.3147288 ,  0.23121512, -0.10979456]],\n       dtype=float32)>,\n <tf.Variable 'my_model/dense_1/bias:0' shape=(10,) dtype=float32, numpy=\n array([0.00100002, 0.00100002, 0.00100002, 0.00100002, 0.00100002,\n        0.00100002, 0.00100002, 0.00100002, 0.00100002, 0.00100002],\n       dtype=float32)>]"
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "model.trainable_variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "apply_gradients(grads_and_vars, name=None)\n",
    "```\n",
    "* 作用：把计算出来的梯度更新到变量上面去。\n",
    "* 参数含义:\n",
    "    * grads_and_vars: (gradient, variable) 对的列表.\n",
    "    * name: 操作名"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 案例2：使用GradientTape自定义训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(tf.keras.Model):\n",
    "\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(MyModel, self).__init__(name='my_model')\n",
    "        self.num_classes = num_classes\n",
    "        # 定义自己需要的层\n",
    "        self.dense_1 = tf.keras.layers.Dense(32, activation='relu')\n",
    "        self.dense_2 = tf.keras.layers.Dense(num_classes)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        #定义前向传播\n",
    "        # 使用在 (in `__init__`)定义的层\n",
    "        x = self.dense_1(inputs)\n",
    "        return self.dense_2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "data = np.random.random((1000, 32))\n",
    "labels = np.random.random((1000, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyModel(num_classes=10)\n",
    "\n",
    "# Instantiate an optimizer.\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=1e-3)\n",
    "# Instantiate a loss function.\n",
    "loss_fn = tf.keras.losses.CategoricalCrossentropy()\n",
    "\n",
    "# Prepare the training dataset.\n",
    "batch_size = 64\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((data, labels))\n",
    "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epoch\n",
    "# batch_size\n",
    "# tape 求梯度  梯度更新"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Start of epoch 0\nWARNING:tensorflow:Layer my_model is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n\nIf you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n\nTo change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n\nTraining loss (for one batch) at step 0: 39.01036071777344\nSeen so far: 64 samples\nStart of epoch 1\nTraining loss (for one batch) at step 0: 37.70331573486328\nSeen so far: 64 samples\nStart of epoch 2\nTraining loss (for one batch) at step 0: 24.322628021240234\nSeen so far: 64 samples\n"
    }
   ],
   "source": [
    "epochs = 3\n",
    "for epoch in range(epochs):\n",
    "    print('Start of epoch %d' % (epoch,))\n",
    "    \n",
    "    # 遍历数据集的batch_size\n",
    "    for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
    "        \n",
    "        # 打开GradientTape以记录正向传递期间运行的操作，这将启用自动区分。\n",
    "        with tf.GradientTape() as tape:\n",
    "\n",
    "            # 运行该模型的前向传播。 模型应用于其输入的操作将记录在GradientTape上。\n",
    "            logits = model(x_batch_train, training=True)  # 这个minibatch的预测值\n",
    "\n",
    "            # 计算这个minibatch的损失值\n",
    "            loss_value = loss_fn(y_batch_train, logits)\n",
    "\n",
    "        # 使用GradientTape自动获取可训练变量相对于损失的梯度。\n",
    "        grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "\n",
    "        # 通过更新变量的值来最大程度地减少损失，从而执行梯度下降的一步。\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "\n",
    "        # 每200 batches打印一次.\n",
    "        if step % 200 == 0:\n",
    "            print('Training loss (for one batch) at step %s: %s' % (step, float(loss_value)))\n",
    "            print('Seen so far: %s samples' % ((step + 1) * 64))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "案例二相当于`model.fit()`，但是缺少评估函数`metrics`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 案例3：使用GradientTape自定义训练模型进阶（加入评估函数）\n",
    "让我们将metric添加到组合中。下面可以在从头开始编写的训练循环中随时使用内置指标（或编写的自定义指标）。流程如下：\n",
    "\n",
    "- 在循环开始时初始化metrics\n",
    "- metric.update_state()：每batch之后更新\n",
    "- metric.result()：需要显示metrics的当前值时调用\n",
    "- metric.reset_states()：需要清除metrics状态时重置（通常在每个epoch的结尾）\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(tf.keras.Model):\n",
    "\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(MyModel, self).__init__(name='my_model')\n",
    "        self.num_classes = num_classes\n",
    "        # 定义自己需要的层\n",
    "        self.dense_1 = tf.keras.layers.Dense(32, activation='relu')\n",
    "        self.dense_2 = tf.keras.layers.Dense(num_classes)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        #定义前向传播\n",
    "        # 使用在 (in `__init__`)定义的层\n",
    "        x = self.dense_1(inputs)\n",
    "        return self.dense_2(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "x_train = np.random.random((1000, 32))\n",
    "y_train = np.random.random((1000, 10))\n",
    "x_val = np.random.random((200, 32))\n",
    "y_val = np.random.random((200, 10))\n",
    "x_test = np.random.random((200, 32))\n",
    "y_test = np.random.random((200, 10))\n",
    "\n",
    "\n",
    "# 优化器\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=1e-3)\n",
    "# 损失函数\n",
    "loss_fn = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "# 准备metrics函数\n",
    "train_acc_metric = tf.keras.metrics.CategoricalAccuracy()\n",
    "val_acc_metric = tf.keras.metrics.CategoricalAccuracy()\n",
    "\n",
    "# 准备训练数据集\n",
    "batch_size = 64\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(batch_size)\n",
    "\n",
    "# 准备测试数据集\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n",
    "val_dataset = val_dataset.batch(64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "进行几个epoch运行训练循环："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Start of epoch 0\nWARNING:tensorflow:Layer my_model is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n\nIf you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n\nTo change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n\nTraining acc over epoch: 0.10199999809265137\nValidation acc: 0.11999999731779099\nStart of epoch 1\nTraining acc over epoch: 0.10000000149011612\nValidation acc: 0.11999999731779099\nStart of epoch 2\nTraining acc over epoch: 0.10000000149011612\nValidation acc: 0.14000000059604645\n"
    }
   ],
   "source": [
    "model = MyModel(num_classes=10)\n",
    "epochs = 3\n",
    "for epoch in range(epochs):\n",
    "    print('Start of epoch %d' % (epoch,))\n",
    "\n",
    "    # 遍历数据集的batch_size\n",
    "    for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
    "        \n",
    "        # 一个batch\n",
    "        with tf.GradientTape() as tape:\n",
    "            logits = model(x_batch_train)\n",
    "            loss_value = loss_fn(y_batch_train, logits)\n",
    "        grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "\n",
    "        # 更新训练集的metrics\n",
    "        train_acc_metric(y_batch_train, logits)     \n",
    "            \n",
    "            \n",
    "    # 在每个epoch结束时显示metrics。\n",
    "    train_acc = train_acc_metric.result()\n",
    "    print('Training acc over epoch: %s' % (float(train_acc),))\n",
    "    # 在每个epoch结束时重置训练指标\n",
    "    train_acc_metric.reset_states()  #!!!!!!!!!!!!!!! 一定要重置\n",
    "\n",
    "    # 在每个epoch结束时运行一个验证集。\n",
    "    for x_batch_val, y_batch_val in val_dataset:\n",
    "        val_logits = model(x_batch_val)\n",
    "        # 更新验证集merics\n",
    "        val_acc_metric(y_batch_val, val_logits)\n",
    "    val_acc = val_acc_metric.result()\n",
    "    print('Validation acc: %s' % (float(val_acc),))\n",
    "    val_acc_metric.reset_states()\n",
    "\n",
    "    #显示测试集\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](media/大纲.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}