{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 案例一、keras版本模型训练\n",
    "相关函数\n",
    "- 构建模型（顺序模型、函数式模型、子类模型）\n",
    "- 模型训练：`model.fit()`\n",
    "- 模型验证：`model.evaluate()`\n",
    "- 模型预测：`model.predict()`\n",
    "\n",
    "### 1.1 构建模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tf.keras.Input(shape=(32,))  # batch_size=32，数据维度32\n",
    "x = tf.keras.layers.Dense(64, activation='relu')(inputs)  # 64个神经元\n",
    "x = tf.keras.layers.Dense(64, activation='relu')(x)  # 63个神经元\n",
    "predictions = tf.keras.layers.Dense(10)(x)  # 输出是10类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#- inputs(模型输入)\n",
    "#- output(模型输出)\n",
    "\n",
    "model = tf.keras.Model(inputs=inputs, outputs=predictions)\n",
    "\n",
    "# 指定损失函数 (loss) tf.keras.optimizers.RMSprop\n",
    "# 优化器 (optimizer) tf.keras.losses.SparseCategoricalCrossentropy\n",
    "# 指标 (metrics) ['accuracy'] \n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(0.001), #优化器\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), #损失函数\n",
    "              metrics=['accuracy']) #评估函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "提供[许多内置的优化器，损失和指标](https://www.tensorflow.org/guide/keras/train_and_evaluate#many_built-in_optimizers_losses_and_metrics_are_available)\n",
    "通常，不必从头开始创建自己的损失，指标或优化函数，因为所需的可能已经是Keras API的一部分：\n",
    "\n",
    "优化器：\n",
    "- SGD() （有或没有动量）\n",
    "- RMSprop()\n",
    "- Adam()\n",
    "\n",
    "损失：\n",
    "- MeanSquaredError()\n",
    "- KLDivergence()\n",
    "- CosineSimilarity()\n",
    "\n",
    "指标：\n",
    "- AUC()\n",
    "- Precision()\n",
    "- Recall()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "另外，如果想用上述的默认设置，那么在很多情况下，可以通过字符串标识符指定优化器，损失和指标："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 构建数据集\n",
    "#\n",
    "import numpy as np\n",
    "x_train = np.random.random((1000, 32))\n",
    "y_train = np.random.randint(10, size=(1000, ))\n",
    "\n",
    "x_val = np.random.random((200, 32))\n",
    "y_val = np.random.randint(10, size=(200, ))\n",
    "\n",
    "x_test = np.random.random((200, 32))\n",
    "y_test = np.random.randint(10, size=(200, ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 模型训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过将数据切成大小为“ batch_size”的“批”来训练模型，并针对给定数量的“epoch”重复遍历整个数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method fit in module tensorflow.python.keras.engine.training:\n",
      "\n",
      "fit(x=None, y=None, batch_size=None, epochs=1, verbose=1, callbacks=None, validation_split=0.0, validation_data=None, shuffle=True, class_weight=None, sample_weight=None, initial_epoch=0, steps_per_epoch=None, validation_steps=None, validation_batch_size=None, validation_freq=1, max_queue_size=10, workers=1, use_multiprocessing=False) method of tensorflow.python.keras.engine.training.Model instance\n",
      "    Trains the model for a fixed number of epochs (iterations on a dataset).\n",
      "    \n",
      "    Arguments:\n",
      "        x: Input data. It could be:\n",
      "          - A Numpy array (or array-like), or a list of arrays\n",
      "            (in case the model has multiple inputs).\n",
      "          - A TensorFlow tensor, or a list of tensors\n",
      "            (in case the model has multiple inputs).\n",
      "          - A dict mapping input names to the corresponding array/tensors,\n",
      "            if the model has named inputs.\n",
      "          - A `tf.data` dataset. Should return a tuple\n",
      "            of either `(inputs, targets)` or\n",
      "            `(inputs, targets, sample_weights)`.\n",
      "          - A generator or `keras.utils.Sequence` returning `(inputs, targets)`\n",
      "            or `(inputs, targets, sample_weights)`.\n",
      "          A more detailed description of unpacking behavior for iterator types\n",
      "          (Dataset, generator, Sequence) is given below.\n",
      "        y: Target data. Like the input data `x`,\n",
      "          it could be either Numpy array(s) or TensorFlow tensor(s).\n",
      "          It should be consistent with `x` (you cannot have Numpy inputs and\n",
      "          tensor targets, or inversely). If `x` is a dataset, generator,\n",
      "          or `keras.utils.Sequence` instance, `y` should\n",
      "          not be specified (since targets will be obtained from `x`).\n",
      "        batch_size: Integer or `None`.\n",
      "            Number of samples per gradient update.\n",
      "            If unspecified, `batch_size` will default to 32.\n",
      "            Do not specify the `batch_size` if your data is in the\n",
      "            form of datasets, generators, or `keras.utils.Sequence` instances\n",
      "            (since they generate batches).\n",
      "        epochs: Integer. Number of epochs to train the model.\n",
      "            An epoch is an iteration over the entire `x` and `y`\n",
      "            data provided.\n",
      "            Note that in conjunction with `initial_epoch`,\n",
      "            `epochs` is to be understood as \"final epoch\".\n",
      "            The model is not trained for a number of iterations\n",
      "            given by `epochs`, but merely until the epoch\n",
      "            of index `epochs` is reached.\n",
      "        verbose: 0, 1, or 2. Verbosity mode.\n",
      "            0 = silent, 1 = progress bar, 2 = one line per epoch.\n",
      "            Note that the progress bar is not particularly useful when\n",
      "            logged to a file, so verbose=2 is recommended when not running\n",
      "            interactively (eg, in a production environment).\n",
      "        callbacks: List of `keras.callbacks.Callback` instances.\n",
      "            List of callbacks to apply during training.\n",
      "            See `tf.keras.callbacks`.\n",
      "        validation_split: Float between 0 and 1.\n",
      "            Fraction of the training data to be used as validation data.\n",
      "            The model will set apart this fraction of the training data,\n",
      "            will not train on it, and will evaluate\n",
      "            the loss and any model metrics\n",
      "            on this data at the end of each epoch.\n",
      "            The validation data is selected from the last samples\n",
      "            in the `x` and `y` data provided, before shuffling. This argument is\n",
      "            not supported when `x` is a dataset, generator or\n",
      "           `keras.utils.Sequence` instance.\n",
      "        validation_data: Data on which to evaluate\n",
      "            the loss and any model metrics at the end of each epoch.\n",
      "            The model will not be trained on this data.\n",
      "            `validation_data` will override `validation_split`.\n",
      "            `validation_data` could be:\n",
      "              - tuple `(x_val, y_val)` of Numpy arrays or tensors\n",
      "              - tuple `(x_val, y_val, val_sample_weights)` of Numpy arrays\n",
      "              - dataset\n",
      "            For the first two cases, `batch_size` must be provided.\n",
      "            For the last case, `validation_steps` could be provided.\n",
      "            Note that `validation_data` does not support all the data types that\n",
      "            are supported in `x`, eg, dict, generator or `keras.utils.Sequence`.\n",
      "        shuffle: Boolean (whether to shuffle the training data\n",
      "            before each epoch) or str (for 'batch'). This argument is ignored\n",
      "            when `x` is a generator. 'batch' is a special option for dealing\n",
      "            with the limitations of HDF5 data; it shuffles in batch-sized\n",
      "            chunks. Has no effect when `steps_per_epoch` is not `None`.\n",
      "        class_weight: Optional dictionary mapping class indices (integers)\n",
      "            to a weight (float) value, used for weighting the loss function\n",
      "            (during training only).\n",
      "            This can be useful to tell the model to\n",
      "            \"pay more attention\" to samples from\n",
      "            an under-represented class.\n",
      "        sample_weight: Optional Numpy array of weights for\n",
      "            the training samples, used for weighting the loss function\n",
      "            (during training only). You can either pass a flat (1D)\n",
      "            Numpy array with the same length as the input samples\n",
      "            (1:1 mapping between weights and samples),\n",
      "            or in the case of temporal data,\n",
      "            you can pass a 2D array with shape\n",
      "            `(samples, sequence_length)`,\n",
      "            to apply a different weight to every timestep of every sample.\n",
      "            In this case you should make sure to specify\n",
      "            `sample_weight_mode=\"temporal\"` in `compile()`. This argument is not\n",
      "            supported when `x` is a dataset, generator, or\n",
      "           `keras.utils.Sequence` instance, instead provide the sample_weights\n",
      "            as the third element of `x`.\n",
      "        initial_epoch: Integer.\n",
      "            Epoch at which to start training\n",
      "            (useful for resuming a previous training run).\n",
      "        steps_per_epoch: Integer or `None`.\n",
      "            Total number of steps (batches of samples)\n",
      "            before declaring one epoch finished and starting the\n",
      "            next epoch. When training with input tensors such as\n",
      "            TensorFlow data tensors, the default `None` is equal to\n",
      "            the number of samples in your dataset divided by\n",
      "            the batch size, or 1 if that cannot be determined. If x is a\n",
      "            `tf.data` dataset, and 'steps_per_epoch'\n",
      "            is None, the epoch will run until the input dataset is exhausted.\n",
      "            When passing an infinitely repeating dataset, you must specify the\n",
      "            `steps_per_epoch` argument. This argument is not supported with\n",
      "            array inputs.\n",
      "        validation_steps: Only relevant if `validation_data` is provided and\n",
      "            is a `tf.data` dataset. Total number of steps (batches of\n",
      "            samples) to draw before stopping when performing validation\n",
      "            at the end of every epoch. If 'validation_steps' is None, validation\n",
      "            will run until the `validation_data` dataset is exhausted. In the\n",
      "            case of an infinitely repeated dataset, it will run into an\n",
      "            infinite loop. If 'validation_steps' is specified and only part of\n",
      "            the dataset will be consumed, the evaluation will start from the\n",
      "            beginning of the dataset at each epoch. This ensures that the same\n",
      "            validation samples are used every time.\n",
      "        validation_batch_size: Integer or `None`.\n",
      "            Number of samples per validation batch.\n",
      "            If unspecified, will default to `batch_size`.\n",
      "            Do not specify the `validation_batch_size` if your data is in the\n",
      "            form of datasets, generators, or `keras.utils.Sequence` instances\n",
      "            (since they generate batches).\n",
      "        validation_freq: Only relevant if validation data is provided. Integer\n",
      "            or `collections_abc.Container` instance (e.g. list, tuple, etc.).\n",
      "            If an integer, specifies how many training epochs to run before a\n",
      "            new validation run is performed, e.g. `validation_freq=2` runs\n",
      "            validation every 2 epochs. If a Container, specifies the epochs on\n",
      "            which to run validation, e.g. `validation_freq=[1, 2, 10]` runs\n",
      "            validation at the end of the 1st, 2nd, and 10th epochs.\n",
      "        max_queue_size: Integer. Used for generator or `keras.utils.Sequence`\n",
      "            input only. Maximum size for the generator queue.\n",
      "            If unspecified, `max_queue_size` will default to 10.\n",
      "        workers: Integer. Used for generator or `keras.utils.Sequence` input\n",
      "            only. Maximum number of processes to spin up\n",
      "            when using process-based threading. If unspecified, `workers`\n",
      "            will default to 1. If 0, will execute the generator on the main\n",
      "            thread.\n",
      "        use_multiprocessing: Boolean. Used for generator or\n",
      "            `keras.utils.Sequence` input only. If `True`, use process-based\n",
      "            threading. If unspecified, `use_multiprocessing` will default to\n",
      "            `False`. Note that because this implementation relies on\n",
      "            multiprocessing, you should not pass non-picklable arguments to\n",
      "            the generator as they can't be passed easily to children processes.\n",
      "    \n",
      "    Unpacking behavior for iterator-like inputs:\n",
      "        A common pattern is to pass a tf.data.Dataset, generator, or\n",
      "      tf.keras.utils.Sequence to the `x` argument of fit, which will in fact\n",
      "      yield not only features (x) but optionally targets (y) and sample weights.\n",
      "      Keras requires that the output of such iterator-likes be unambiguous. The\n",
      "      iterator should return a tuple of length 1, 2, or 3, where the optional\n",
      "      second and third elements will be used for y and sample_weight\n",
      "      respectively. Any other type provided will be wrapped in a length one\n",
      "      tuple, effectively treating everything as 'x'. When yielding dicts, they\n",
      "      should still adhere to the top-level tuple structure.\n",
      "      e.g. `({\"x0\": x0, \"x1\": x1}, y)`. Keras will not attempt to separate\n",
      "      features, targets, and weights from the keys of a single dict.\n",
      "        A notable unsupported data type is the namedtuple. The reason is that\n",
      "      it behaves like both an ordered datatype (tuple) and a mapping\n",
      "      datatype (dict). So given a namedtuple of the form:\n",
      "          `namedtuple(\"example_tuple\", [\"y\", \"x\"])`\n",
      "      it is ambiguous whether to reverse the order of the elements when\n",
      "      interpreting the value. Even worse is a tuple of the form:\n",
      "          `namedtuple(\"other_tuple\", [\"x\", \"y\", \"z\"])`\n",
      "      where it is unclear if the tuple was intended to be unpacked into x, y,\n",
      "      and sample_weight or passed through as a single element to `x`. As a\n",
      "      result the data processing code will simply raise a ValueError if it\n",
      "      encounters a namedtuple. (Along with instructions to remedy the issue.)\n",
      "    \n",
      "    Returns:\n",
      "        A `History` object. Its `History.history` attribute is\n",
      "        a record of training loss values and metrics values\n",
      "        at successive epochs, as well as validation loss values\n",
      "        and validation metrics values (if applicable).\n",
      "    \n",
      "    Raises:\n",
      "        RuntimeError: If the model was never compiled.\n",
      "        ValueError: In case of mismatch between the provided input data\n",
      "            and what the model expects.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(model.fit)\n",
    "#N/batch_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 2.3250 - accuracy: 0.0920 - val_loss: 2.3105 - val_accuracy: 0.1000\n",
      "Epoch 2/5\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2.2959 - accuracy: 0.1290 - val_loss: 2.3119 - val_accuracy: 0.1050\n",
      "Epoch 3/5\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2.2856 - accuracy: 0.1360 - val_loss: 2.3146 - val_accuracy: 0.1200\n",
      "Epoch 4/5\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2.2769 - accuracy: 0.1480 - val_loss: 2.3138 - val_accuracy: 0.0850\n",
      "Epoch 5/5\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2.2681 - accuracy: 0.1650 - val_loss: 2.3217 - val_accuracy: 0.0750\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1d9851e3d30>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, batch_size=32, epochs=5, validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "自动划分验证集\n",
    "\n",
    "在前面的例子中，我们使用validation_data参数将Numpy数组的元组传递(x_val, y_val)给模型，以在每个时期结束时评估验证损失和验证指标。\n",
    "\n",
    "还有一个选择：参数validation_split允许您自动保留部分训练数据以供验证。参数值代表要保留用于验证的数据的一部分，因此应将其设置为大于0且小于1的数字。例如，validation_split=0.2表示“使用20％的数据进行验证”，而validation_split=0.6表示“使用60％的数据用于验证”。\n",
    "\n",
    "验证的计算方法是在进行任何改组之前，对fit调用接收到的数组进行最后x％的采样。\n",
    "\n",
    "注意，只能validation_split在使用Numpy数据进行训练时使用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - 0s 9ms/step - loss: 2.2538 - accuracy: 0.1787 - val_loss: 2.2658 - val_accuracy: 0.1800\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1d98bf5d6a0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, batch_size=64, validation_split=0.2, epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 模型验证\n",
    "\n",
    "返回 test loss 和metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# Evaluate on test data\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.3222 - accuracy: 0.0850\n",
      "test loss, test acc: [2.322190284729004, 0.08500000089406967]\n",
      "\n",
      "# Generate predictions for 3 samples\n",
      "predictions shape: (3, 10)\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test data using `evaluate`\n",
    "print('\\n# Evaluate on test data')\n",
    "results = model.evaluate(x_test, y_test, batch_size=128)\n",
    "print('test loss, test acc:', results)\n",
    "\n",
    "# Generate predictions (probabilities -- the output of the last layer)\n",
    "# on new data using `predict`\n",
    "print('\\n# Generate predictions for 3 samples')\n",
    "predictions = model.predict(x_test[:3])\n",
    "print('predictions shape:', predictions.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 案例2、使用样本加权和类别加权"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "除了输入数据和目标数据外，还可以在使用时将样本权重或类权重传递给模型fit：\n",
    "\n",
    "从Numpy数据进行训练时：通过sample_weight和class_weight参数。\n",
    "从数据集训练时：通过使数据集返回一个元组(input_batch, target_batch, sample_weight_batch)。\n",
    "“样本权重”数组是一个数字数组，用于指定批次中每个样本在计算总损失时应具有的权重。它通常用于不平衡的分类问题中（这种想法是为很少见的班级赋予更多的权重）。当所使用的权重为1和0时，该数组可用作损失函数的掩码（完全丢弃某些样本对总损失的贡献）。\n",
    "\n",
    "“类别权重”字典是同一概念的一个更具体的实例：它将类别索引映射到应该用于属于该类别的样本的样本权重。例如，如果在数据中类“ 0”的表示量比类“ 1”的表示量少两倍，则可以使用class_weight={0: 1., 1: 0.5}。\n",
    "\n",
    "这是一个Numpy示例，其中我们使用类权重或样本权重来更加**重视第5类的正确分类**。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 构建模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_uncompiled_model():\n",
    "    inputs = tf.keras.Input(shape=(32,), name='digits')\n",
    "    x = tf.keras.layers.Dense(64, activation='relu', name='dense_1')(inputs)\n",
    "    x = tf.keras.layers.Dense(64, activation='relu', name='dense_2')(x)\n",
    "    outputs = tf.keras.layers.Dense(10, name='predictions')(x)\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_compiled_model():\n",
    "    model = get_uncompiled_model()\n",
    "    model.compile(optimizer=tf.keras.optimizers.RMSprop(learning_rate=1e-3),\n",
    "                  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                  metrics=['sparse_categorical_accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 模型训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "模型加权"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit with class weight\n",
      "Epoch 1/4\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 2.5094 - sparse_categorical_accuracy: 0.1010\n",
      "Epoch 2/4\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 2.4874 - sparse_categorical_accuracy: 0.1010\n",
      "Epoch 3/4\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 2.4775 - sparse_categorical_accuracy: 0.1060\n",
      "Epoch 4/4\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 2.4699 - sparse_categorical_accuracy: 0.1030\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1d98af08b38>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 类别5：加权\n",
    "class_weight = {0: 1., 1: 1., 2: 1., 3: 1., 4: 1.,\n",
    "                5: 2.,\n",
    "                6: 1., 7: 1., 8: 1., 9: 1.}\n",
    "\n",
    "\n",
    "print('Fit with class weight')\n",
    "model = get_compiled_model()\n",
    "model.fit(x_train, y_train,\n",
    "          class_weight=class_weight,\n",
    "          batch_size=64,\n",
    "          epochs=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "样本加权"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fit with sample weight\n"
     ]
    }
   ],
   "source": [
    "# Here's the same example using `sample_weight` instead:\n",
    "sample_weight = np.ones(shape=(len(y_train),))\n",
    "\n",
    "sample_weight[y_train == 5] = 2.\n",
    "print('\\nFit with sample weight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 2.5414 - sparse_categorical_accuracy: 0.0890\n",
      "Epoch 2/4\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 2.5057 - sparse_categorical_accuracy: 0.1010\n",
      "Epoch 3/4\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 2.4939 - sparse_categorical_accuracy: 0.1000\n",
      "Epoch 4/4\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 2.4852 - sparse_categorical_accuracy: 0.1050\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1d98d0f6c18>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = get_compiled_model()\n",
    "model.fit(x_train, y_train,\n",
    "          sample_weight=sample_weight,\n",
    "          batch_size=64,\n",
    "          epochs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 案例3、使用回调函数\n",
    "\n",
    "Keras中的回调是在训练期间（在某个时期开始时，在批处理结束时，在某个时期结束时等）在不同时间点调用的对象，这些对象可用于实现以下行为：\n",
    "\n",
    "- 在训练过程中的不同时间点进行验证（除了内置的按时间段验证）\n",
    "\n",
    "- 定期或在超过特定精度阈值时对模型进行检查\n",
    "\n",
    "- 当训练似乎停滞不前时，更改模型的学习率\n",
    "\n",
    "- 当训练似乎停滞不前时，对顶层进行微调\n",
    "\n",
    "在训练结束或超出特定性能阈值时发送电子邮件或即时消息通知\n",
    "等等。\n",
    "回调可以作为列表传递给model.fit："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ModelCheckpoint：定期保存模型。\n",
    "- EarlyStopping：当训练不再改善验证指标时，停止培训。\n",
    "- TensorBoard：定期编写可在TensorBoard中可视化的模型日志（更多详细信\n",
    "息，请参见“可视化”部分）。\n",
    "- CSVLogger：将损失和指标数据流式传输到CSV文件"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 EarlyStopping(早停)\n",
    "- monitor: 被监测的数据。\n",
    "- min_delta: 在被监测的数据中被认为是提升的最小变化， 例如，小于 min_delta 的绝对变化会被认为没有提升。\n",
    "- patience: 没有进步的训练轮数，在这之后训练就会被停止。\n",
    "- verbose: 详细信息模式。\n",
    "- mode: {auto, min, max} 其中之一。 在 min 模式中， 当被监测的数据停止下降，训练就会停止；在 max 模式中，当被监测的数据停止上升，训练就会停止；在 auto 模式中，方向会自动从被监测的数据的名字中判断出来。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(tf.keras.callbacks.EarlyStopping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 800 samples, validate on 200 samples\n",
      "Epoch 1/20\n",
      "800/800 [==============================] - 1s 1ms/sample - loss: 2.3292 - sparse_categorical_accuracy: 0.0938 - val_loss: 2.3100 - val_sparse_categorical_accuracy: 0.0750\n",
      "Epoch 2/20\n",
      "800/800 [==============================] - 0s 100us/sample - loss: 2.3007 - sparse_categorical_accuracy: 0.1238 - val_loss: 2.3065 - val_sparse_categorical_accuracy: 0.0800\n",
      "Epoch 3/20\n",
      "800/800 [==============================] - 0s 97us/sample - loss: 2.2889 - sparse_categorical_accuracy: 0.1312 - val_loss: 2.3065 - val_sparse_categorical_accuracy: 0.1100\n",
      "Epoch 00003: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x29c01915a90>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = get_compiled_model()\n",
    "\n",
    "# list\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(\n",
    "        # 当‘val_loss’不再下降时候停止训练 \n",
    "        monitor='val_loss',\n",
    "        # “不再下降”被定义为“减少不超过1e-2”\n",
    "        min_delta=1e-2,\n",
    "        # “不再改善”进一步定义为“至少2个epoch”\n",
    "        patience=2,\n",
    "        verbose=1)\n",
    "]\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          epochs=20,\n",
    "          batch_size=64,\n",
    "          callbacks=callbacks,\n",
    "          validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 许多内置的回调可用\n",
    "- ModelCheckpoint：定期保存模型。\n",
    "- EarlyStopping：当培训不再改善验证指标时，停止培训。\n",
    "- TensorBoard：定期编写可在TensorBoard中可视化的模型日志（更多详细信息，请参见“可视化”部分）。\n",
    "- CSVLogger：将损失和指标数据流式传输到CSV文件。\n",
    "等等"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 checkpoint模型\n",
    "在相对较大的数据集上训练模型时，至关重要的是要定期保存模型的checkpoint。\n",
    "\n",
    "最简单的方法是使用ModelCheckpoint回调："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class ModelCheckpoint in module tensorflow.python.keras.callbacks:\n",
      "\n",
      "class ModelCheckpoint(Callback)\n",
      " |  Callback to save the Keras model or model weights at some frequency.\n",
      " |  \n",
      " |  `ModelCheckpoint` callback is used in conjunction with training using\n",
      " |  `model.fit()` to save a model or weights (in a checkpoint file) at some\n",
      " |  interval, so the model or weights can be loaded later to continue the training\n",
      " |  from the state saved.\n",
      " |  \n",
      " |  A few options this callback provides include:\n",
      " |  \n",
      " |  - Whether to only keep the model that has achieved the \"best performance\" so\n",
      " |    far, or whether to save the model at the end of every epoch regardless of\n",
      " |    performance.\n",
      " |  - Definition of 'best'; which quantity to monitor and whether it should be\n",
      " |    maximized or minimized.\n",
      " |  - The frequency it should save at. Currently, the callback supports saving at\n",
      " |    the end of every epoch, or after a fixed number of training batches.\n",
      " |  - Whether only weights are saved, or the whole model is saved.\n",
      " |  \n",
      " |  Example:\n",
      " |  \n",
      " |  ```python\n",
      " |  EPOCHS = 10\n",
      " |  checkpoint_filepath = '/tmp/checkpoint'\n",
      " |  model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
      " |      filepath=checkpoint_filepath,\n",
      " |      save_weights_only=True,\n",
      " |      monitor='val_acc',\n",
      " |      mode='max',\n",
      " |      save_best_only=True)\n",
      " |  \n",
      " |  # Model weights are saved at the end of every epoch, if it's the best seen\n",
      " |  # so far.\n",
      " |  model.fit(epochs=EPOCHS, callbacks=[model_checkpoint_callback])\n",
      " |  \n",
      " |  # The model weights (that are considered the best) are loaded into the model.\n",
      " |  model.load_weights(checkpoint_filepath)\n",
      " |  ```\n",
      " |  \n",
      " |  Arguments:\n",
      " |      filepath: string, path to save the model file. `filepath` can contain\n",
      " |        named formatting options, which will be filled the value of `epoch` and\n",
      " |        keys in `logs` (passed in `on_epoch_end`). For example: if `filepath` is\n",
      " |        `weights.{epoch:02d}-{val_loss:.2f}.hdf5`, then the model checkpoints\n",
      " |        will be saved with the epoch number and the validation loss in the\n",
      " |        filename.\n",
      " |      monitor: quantity to monitor.\n",
      " |      verbose: verbosity mode, 0 or 1.\n",
      " |      save_best_only: if `save_best_only=True`, the latest best model according\n",
      " |        to the quantity monitored will not be overwritten.\n",
      " |        If `filepath` doesn't contain formatting options like `{epoch}` then\n",
      " |        `filepath` will be overwritten by each new better model.\n",
      " |      mode: one of {auto, min, max}. If `save_best_only=True`, the decision to\n",
      " |        overwrite the current save file is made based on either the maximization\n",
      " |        or the minimization of the monitored quantity. For `val_acc`, this\n",
      " |        should be `max`, for `val_loss` this should be `min`, etc. In `auto`\n",
      " |        mode, the direction is automatically inferred from the name of the\n",
      " |        monitored quantity.\n",
      " |      save_weights_only: if True, then only the model's weights will be saved\n",
      " |        (`model.save_weights(filepath)`), else the full model is saved\n",
      " |        (`model.save(filepath)`).\n",
      " |      save_freq: `'epoch'` or integer. When using `'epoch'`, the callback saves\n",
      " |        the model after each epoch. When using integer, the callback saves the\n",
      " |        model at end of this many batches. Note that if the saving isn't aligned\n",
      " |        to epochs, the monitored metric may potentially be less reliable (it\n",
      " |        could reflect as little as 1 batch, since the metrics get reset every\n",
      " |        epoch). Defaults to `'epoch'`\n",
      " |      **kwargs: Additional arguments for backwards compatibility. Possible key\n",
      " |        is `period`.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      ModelCheckpoint\n",
      " |      Callback\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, filepath, monitor='val_loss', verbose=0, save_best_only=False, save_weights_only=False, mode='auto', save_freq='epoch', **kwargs)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  on_batch_end(self, batch, logs=None)\n",
      " |      A backwards compatibility alias for `on_train_batch_end`.\n",
      " |  \n",
      " |  on_epoch_begin(self, epoch, logs=None)\n",
      " |      Called at the start of an epoch.\n",
      " |      \n",
      " |      Subclasses should override for any actions to run. This function should only\n",
      " |      be called during TRAIN mode.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          epoch: integer, index of epoch.\n",
      " |          logs: dict. Currently no data is passed to this argument for this method\n",
      " |            but that may change in the future.\n",
      " |  \n",
      " |  on_epoch_end(self, epoch, logs=None)\n",
      " |      Called at the end of an epoch.\n",
      " |      \n",
      " |      Subclasses should override for any actions to run. This function should only\n",
      " |      be called during TRAIN mode.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          epoch: integer, index of epoch.\n",
      " |          logs: dict, metric results for this training epoch, and for the\n",
      " |            validation epoch if validation is performed. Validation result keys\n",
      " |            are prefixed with `val_`.\n",
      " |  \n",
      " |  on_train_begin(self, logs=None)\n",
      " |      Called at the beginning of training.\n",
      " |      \n",
      " |      Subclasses should override for any actions to run.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          logs: dict. Currently no data is passed to this argument for this method\n",
      " |            but that may change in the future.\n",
      " |  \n",
      " |  on_train_end(self, logs=None)\n",
      " |      Called at the end of training.\n",
      " |      \n",
      " |      Subclasses should override for any actions to run.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          logs: dict. Currently no data is passed to this argument for this method\n",
      " |            but that may change in the future.\n",
      " |  \n",
      " |  set_model(self, model)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from Callback:\n",
      " |  \n",
      " |  on_batch_begin(self, batch, logs=None)\n",
      " |      A backwards compatibility alias for `on_train_batch_begin`.\n",
      " |  \n",
      " |  on_predict_batch_begin(self, batch, logs=None)\n",
      " |      Called at the beginning of a batch in `predict` methods.\n",
      " |      \n",
      " |      Subclasses should override for any actions to run.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          batch: integer, index of batch within the current epoch.\n",
      " |          logs: dict. Has keys `batch` and `size` representing the current batch\n",
      " |            number and the size of the batch.\n",
      " |  \n",
      " |  on_predict_batch_end(self, batch, logs=None)\n",
      " |      Called at the end of a batch in `predict` methods.\n",
      " |      \n",
      " |      Subclasses should override for any actions to run.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          batch: integer, index of batch within the current epoch.\n",
      " |          logs: dict. Metric results for this batch.\n",
      " |  \n",
      " |  on_predict_begin(self, logs=None)\n",
      " |      Called at the beginning of prediction.\n",
      " |      \n",
      " |      Subclasses should override for any actions to run.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          logs: dict. Currently no data is passed to this argument for this method\n",
      " |            but that may change in the future.\n",
      " |  \n",
      " |  on_predict_end(self, logs=None)\n",
      " |      Called at the end of prediction.\n",
      " |      \n",
      " |      Subclasses should override for any actions to run.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          logs: dict. Currently no data is passed to this argument for this method\n",
      " |            but that may change in the future.\n",
      " |  \n",
      " |  on_test_batch_begin(self, batch, logs=None)\n",
      " |      Called at the beginning of a batch in `evaluate` methods.\n",
      " |      \n",
      " |      Also called at the beginning of a validation batch in the `fit`\n",
      " |      methods, if validation data is provided.\n",
      " |      \n",
      " |      Subclasses should override for any actions to run.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          batch: integer, index of batch within the current epoch.\n",
      " |          logs: dict. Has keys `batch` and `size` representing the current batch\n",
      " |            number and the size of the batch.\n",
      " |  \n",
      " |  on_test_batch_end(self, batch, logs=None)\n",
      " |      Called at the end of a batch in `evaluate` methods.\n",
      " |      \n",
      " |      Also called at the end of a validation batch in the `fit`\n",
      " |      methods, if validation data is provided.\n",
      " |      \n",
      " |      Subclasses should override for any actions to run.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          batch: integer, index of batch within the current epoch.\n",
      " |          logs: dict. Metric results for this batch.\n",
      " |  \n",
      " |  on_test_begin(self, logs=None)\n",
      " |      Called at the beginning of evaluation or validation.\n",
      " |      \n",
      " |      Subclasses should override for any actions to run.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          logs: dict. Currently no data is passed to this argument for this method\n",
      " |            but that may change in the future.\n",
      " |  \n",
      " |  on_test_end(self, logs=None)\n",
      " |      Called at the end of evaluation or validation.\n",
      " |      \n",
      " |      Subclasses should override for any actions to run.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          logs: dict. Currently no data is passed to this argument for this method\n",
      " |            but that may change in the future.\n",
      " |  \n",
      " |  on_train_batch_begin(self, batch, logs=None)\n",
      " |      Called at the beginning of a training batch in `fit` methods.\n",
      " |      \n",
      " |      Subclasses should override for any actions to run.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          batch: integer, index of batch within the current epoch.\n",
      " |          logs: dict. Has keys `batch` and `size` representing the current batch\n",
      " |            number and the size of the batch.\n",
      " |  \n",
      " |  on_train_batch_end(self, batch, logs=None)\n",
      " |      Called at the end of a training batch in `fit` methods.\n",
      " |      \n",
      " |      Subclasses should override for any actions to run.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          batch: integer, index of batch within the current epoch.\n",
      " |          logs: dict. Metric results for this batch.\n",
      " |  \n",
      " |  set_params(self, params)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from Callback:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tf.keras.callbacks.ModelCheckpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 2.3519 - sparse_categorical_accuracy: 0.0625\n",
      "Epoch 00001: val_loss improved from inf to 2.31415, saving model to mymodel_1\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 2.3212 - sparse_categorical_accuracy: 0.0975 - val_loss: 2.3142 - val_sparse_categorical_accuracy: 0.0600\n",
      "Epoch 2/3\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 2.2944 - sparse_categorical_accuracy: 0.1406\n",
      "Epoch 00002: val_loss did not improve from 2.31415\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 2.2989 - sparse_categorical_accuracy: 0.1075 - val_loss: 2.3181 - val_sparse_categorical_accuracy: 0.0700\n",
      "Epoch 3/3\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 2.2871 - sparse_categorical_accuracy: 0.1562\n",
      "Epoch 00003: val_loss did not improve from 2.31415\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 2.2901 - sparse_categorical_accuracy: 0.1238 - val_loss: 2.3157 - val_sparse_categorical_accuracy: 0.0850\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1d98e4ac160>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = get_compiled_model()\n",
    "\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath='mymodel_{epoch}',\n",
    "        # 模型保存路径\n",
    "        #\n",
    "        # 下面的两个参数意味着当且仅当`val_loss`分数提高时，我们才会覆盖当前检查点。\n",
    "        save_best_only=True,\n",
    "        monitor='val_loss',\n",
    "        #加入这个仅仅保存模型权重\n",
    "        save_weights_only=True,\n",
    "        verbose=1)\n",
    "]\n",
    "model.fit(x_train, y_train,\n",
    "          epochs=3,\n",
    "          batch_size=64,\n",
    "          callbacks=callbacks,\n",
    "          validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3、使用回调实现动态学习率调整\n",
    "由于优化程序无法访问验证指标，因此无法使用这些计划对象来实现动态学习率计划（例如，当验证损失不再改善时降低学习率）。\n",
    "\n",
    "但是，回调确实可以访问所有指标，包括验证指标！因此，可以通过使用回调来修改优化程序上的当前学习率，从而实现此模式。实际上，它是作为ReduceLROnPlateau回调内置的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ReduceLROnPlateau参数\n",
    "\n",
    "- monitor: 被监测的指标。\n",
    "- factor: 学习速率被降低的因数。新的学习速率 = 学习速率 * 因数\n",
    "- patience: 没有进步的训练轮数，在这之后训练速率会被降低。\n",
    "- verbose: 整数。0：安静，1：更新信息。\n",
    "- mode: {auto, min, max} 其中之一。如果是 min 模式，学习速率会被降低如果被监测的数据已经停止下降； 在 max 模式，学习塑料会被降低如果被监测的数据已经停止上升； 在 auto 模式，方向会被从被监测的数据中自动推断出来。\n",
    "- min_delta: 衡量新的最佳阈值，仅关注重大变化。\n",
    "- cooldown: 在学习速率被降低之后，重新恢复正常操作之前等待的训练轮数量。\n",
    "- min_lr: 学习速率的下边界。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 2.3360 - sparse_categorical_accuracy: 0.1250\n",
      "Epoch 00001: val_loss improved from inf to 2.30149, saving model to mymodel_1\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 2.3278 - sparse_categorical_accuracy: 0.0913 - val_loss: 2.3015 - val_sparse_categorical_accuracy: 0.1050 - lr: 0.0010\n",
      "Epoch 2/30\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 2.2964 - sparse_categorical_accuracy: 0.1875\n",
      "Epoch 00002: val_loss did not improve from 2.30149\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 2.2991 - sparse_categorical_accuracy: 0.1238 - val_loss: 2.3116 - val_sparse_categorical_accuracy: 0.1000 - lr: 0.0010\n",
      "Epoch 3/30\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 2.2946 - sparse_categorical_accuracy: 0.1094\n",
      "Epoch 00003: val_loss did not improve from 2.30149\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 2.2890 - sparse_categorical_accuracy: 0.1238 - val_loss: 2.3103 - val_sparse_categorical_accuracy: 0.0850 - lr: 0.0010\n",
      "Epoch 4/30\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 2.2715 - sparse_categorical_accuracy: 0.0469\n",
      "Epoch 00004: val_loss did not improve from 2.30149\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 2.2820 - sparse_categorical_accuracy: 0.1375 - val_loss: 2.3118 - val_sparse_categorical_accuracy: 0.1000 - lr: 0.0010\n",
      "Epoch 5/30\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 2.2659 - sparse_categorical_accuracy: 0.1875\n",
      "Epoch 00005: val_loss did not improve from 2.30149\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 2.2720 - sparse_categorical_accuracy: 0.1475 - val_loss: 2.3131 - val_sparse_categorical_accuracy: 0.0800 - lr: 5.0000e-04\n",
      "Epoch 6/30\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 2.2648 - sparse_categorical_accuracy: 0.1562\n",
      "Epoch 00006: val_loss did not improve from 2.30149\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 2.2676 - sparse_categorical_accuracy: 0.1488 - val_loss: 2.3140 - val_sparse_categorical_accuracy: 0.0950 - lr: 5.0000e-04\n",
      "Epoch 7/30\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 2.2786 - sparse_categorical_accuracy: 0.1406\n",
      "Epoch 00007: val_loss did not improve from 2.30149\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 2.2638 - sparse_categorical_accuracy: 0.1525 - val_loss: 2.3146 - val_sparse_categorical_accuracy: 0.0900 - lr: 5.0000e-04\n",
      "Epoch 8/30\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 2.2481 - sparse_categorical_accuracy: 0.1406\n",
      "Epoch 00008: val_loss did not improve from 2.30149\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 2.2588 - sparse_categorical_accuracy: 0.1562 - val_loss: 2.3149 - val_sparse_categorical_accuracy: 0.1000 - lr: 2.5000e-04\n",
      "Epoch 9/30\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 2.2827 - sparse_categorical_accuracy: 0.0938\n",
      "Epoch 00009: val_loss did not improve from 2.30149\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 2.2569 - sparse_categorical_accuracy: 0.1538 - val_loss: 2.3156 - val_sparse_categorical_accuracy: 0.0800 - lr: 2.5000e-04\n",
      "Epoch 10/30\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 2.2700 - sparse_categorical_accuracy: 0.0625\n",
      "Epoch 00010: val_loss did not improve from 2.30149\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 2.2548 - sparse_categorical_accuracy: 0.1625 - val_loss: 2.3165 - val_sparse_categorical_accuracy: 0.1000 - lr: 2.5000e-04\n",
      "Epoch 11/30\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 2.2626 - sparse_categorical_accuracy: 0.1250\n",
      "Epoch 00011: val_loss did not improve from 2.30149\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 2.2523 - sparse_categorical_accuracy: 0.1637 - val_loss: 2.3164 - val_sparse_categorical_accuracy: 0.1050 - lr: 1.2500e-04\n",
      "Epoch 12/30\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 2.2541 - sparse_categorical_accuracy: 0.2188\n",
      "Epoch 00012: val_loss did not improve from 2.30149\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 2.2514 - sparse_categorical_accuracy: 0.1637 - val_loss: 2.3169 - val_sparse_categorical_accuracy: 0.1000 - lr: 1.2500e-04\n",
      "Epoch 13/30\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 2.2427 - sparse_categorical_accuracy: 0.1875\n",
      "Epoch 00013: val_loss did not improve from 2.30149\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 2.2505 - sparse_categorical_accuracy: 0.1688 - val_loss: 2.3168 - val_sparse_categorical_accuracy: 0.0900 - lr: 1.2500e-04\n",
      "Epoch 14/30\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 2.2181 - sparse_categorical_accuracy: 0.2344\n",
      "Epoch 00014: val_loss did not improve from 2.30149\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 2.2491 - sparse_categorical_accuracy: 0.1637 - val_loss: 2.3169 - val_sparse_categorical_accuracy: 0.0900 - lr: 6.2500e-05\n",
      "Epoch 15/30\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 2.3028 - sparse_categorical_accuracy: 0.0938\n",
      "Epoch 00015: val_loss did not improve from 2.30149\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 2.2487 - sparse_categorical_accuracy: 0.1637 - val_loss: 2.3172 - val_sparse_categorical_accuracy: 0.0900 - lr: 6.2500e-05\n",
      "Epoch 16/30\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 2.2496 - sparse_categorical_accuracy: 0.1875\n",
      "Epoch 00016: val_loss did not improve from 2.30149\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 2.2482 - sparse_categorical_accuracy: 0.1637 - val_loss: 2.3175 - val_sparse_categorical_accuracy: 0.0900 - lr: 6.2500e-05\n",
      "Epoch 17/30\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 2.2363 - sparse_categorical_accuracy: 0.1094\n",
      "Epoch 00017: val_loss did not improve from 2.30149\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 2.2475 - sparse_categorical_accuracy: 0.1663 - val_loss: 2.3175 - val_sparse_categorical_accuracy: 0.0900 - lr: 3.1250e-05\n",
      "Epoch 18/30\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 2.2300 - sparse_categorical_accuracy: 0.1875\n",
      "Epoch 00018: val_loss did not improve from 2.30149\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 2.2473 - sparse_categorical_accuracy: 0.1688 - val_loss: 2.3176 - val_sparse_categorical_accuracy: 0.0850 - lr: 3.1250e-05\n",
      "Epoch 19/30\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 2.2287 - sparse_categorical_accuracy: 0.2188\n",
      "Epoch 00019: val_loss did not improve from 2.30149\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 2.2470 - sparse_categorical_accuracy: 0.1688 - val_loss: 2.3178 - val_sparse_categorical_accuracy: 0.0850 - lr: 3.1250e-05\n",
      "Epoch 20/30\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 2.2558 - sparse_categorical_accuracy: 0.1250\n",
      "Epoch 00020: val_loss did not improve from 2.30149\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 2.2467 - sparse_categorical_accuracy: 0.1663 - val_loss: 2.3179 - val_sparse_categorical_accuracy: 0.0850 - lr: 1.5625e-05\n",
      "Epoch 21/30\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 2.2526 - sparse_categorical_accuracy: 0.1094\n",
      "Epoch 00021: val_loss did not improve from 2.30149\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 2.2466 - sparse_categorical_accuracy: 0.1675 - val_loss: 2.3179 - val_sparse_categorical_accuracy: 0.0850 - lr: 1.5625e-05\n",
      "Epoch 22/30\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 2.2307 - sparse_categorical_accuracy: 0.1719\n",
      "Epoch 00022: val_loss did not improve from 2.30149\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 2.2465 - sparse_categorical_accuracy: 0.1675 - val_loss: 2.3180 - val_sparse_categorical_accuracy: 0.0850 - lr: 1.5625e-05\n",
      "Epoch 23/30\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 2.2367 - sparse_categorical_accuracy: 0.1406\n",
      "Epoch 00023: val_loss did not improve from 2.30149\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 2.2463 - sparse_categorical_accuracy: 0.1675 - val_loss: 2.3180 - val_sparse_categorical_accuracy: 0.0850 - lr: 7.8125e-06\n",
      "Epoch 24/30\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 2.2674 - sparse_categorical_accuracy: 0.1406\n",
      "Epoch 00024: val_loss did not improve from 2.30149\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 2.2463 - sparse_categorical_accuracy: 0.1675 - val_loss: 2.3181 - val_sparse_categorical_accuracy: 0.0850 - lr: 7.8125e-06\n",
      "Epoch 25/30\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 2.2355 - sparse_categorical_accuracy: 0.1875\n",
      "Epoch 00025: val_loss did not improve from 2.30149\n",
      "\n",
      "Epoch 00025: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 2.2462 - sparse_categorical_accuracy: 0.1675 - val_loss: 2.3181 - val_sparse_categorical_accuracy: 0.0850 - lr: 7.8125e-06\n",
      "Epoch 26/30\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 2.2393 - sparse_categorical_accuracy: 0.1875\n",
      "Epoch 00026: val_loss did not improve from 2.30149\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 2.2461 - sparse_categorical_accuracy: 0.1675 - val_loss: 2.3181 - val_sparse_categorical_accuracy: 0.0850 - lr: 3.9063e-06\n",
      "Epoch 27/30\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 2.2472 - sparse_categorical_accuracy: 0.1562\n",
      "Epoch 00027: val_loss did not improve from 2.30149\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 2.2461 - sparse_categorical_accuracy: 0.1675 - val_loss: 2.3181 - val_sparse_categorical_accuracy: 0.0850 - lr: 3.9063e-06\n",
      "Epoch 28/30\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 2.2524 - sparse_categorical_accuracy: 0.1562\n",
      "Epoch 00028: val_loss did not improve from 2.30149\n",
      "\n",
      "Epoch 00028: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 2.2461 - sparse_categorical_accuracy: 0.1675 - val_loss: 2.3181 - val_sparse_categorical_accuracy: 0.0850 - lr: 3.9063e-06\n",
      "Epoch 29/30\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 2.2539 - sparse_categorical_accuracy: 0.1250\n",
      "Epoch 00029: val_loss did not improve from 2.30149\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 2.2460 - sparse_categorical_accuracy: 0.1675 - val_loss: 2.3182 - val_sparse_categorical_accuracy: 0.0850 - lr: 1.9531e-06\n",
      "Epoch 30/30\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 2.2365 - sparse_categorical_accuracy: 0.1875\n",
      "Epoch 00030: val_loss did not improve from 2.30149\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 2.2460 - sparse_categorical_accuracy: 0.1675 - val_loss: 2.3182 - val_sparse_categorical_accuracy: 0.0850 - lr: 1.9531e-06\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1d98f62bdd8>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = get_compiled_model()\n",
    "\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath='mymodel_{epoch}',\n",
    "        # 模型保存路径\n",
    "        # 下面的两个参数意味着当且仅当`val_loss`分数提高时，我们才会覆盖当前检查点。\n",
    "        save_best_only=True,\n",
    "        monitor='val_loss',\n",
    "        #加入这个仅仅保存模型权重\n",
    "        save_weights_only=True,\n",
    "        verbose=1),\n",
    "    \n",
    "    tf.keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor=\"val_sparse_categorical_accuracy\",\n",
    "        verbose=1, \n",
    "        mode='max',\n",
    "        factor=0.5, \n",
    "        patience=3)\n",
    "]\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          epochs=30,\n",
    "          batch_size=64,\n",
    "          callbacks=callbacks,\n",
    "          validation_split=0.2\n",
    "         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 案例4、将数据传递到多输入，多输出模型\n",
    "![](media/多输入多输出.png)\n",
    "在前面的示例中，我们正在考虑一个具有单个输入（shape的张量(32,)）和单个输出（shape的预测张量(10,)）的模型。但是具有多个输入或输出的模型呢？\n",
    "\n",
    "考虑以下模型，该模型具有形状的图像输入(32, 32, 3)（即(height, width, channels)）和形状的时间序列输入(None, 10)（即(timesteps, features)）。我们的模型将具有根据这些输入的组合计算出的两个输出：“得分”（形状(1,)）和五类（形状(5,)）的概率分布。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_input = tf.keras.Input(shape=(32, 32, 3), name='img_input')\n",
    "timeseries_input = tf.keras.Input(shape=(20, 10), name='ts_input')\n",
    "\n",
    "x1 = tf.keras.layers.Conv2D(3, 3)(image_input)\n",
    "x1 = tf.keras.layers.GlobalMaxPooling2D()(x1)\n",
    "\n",
    "\n",
    "x2 = tf.keras.layers.Conv1D(3, 3)(timeseries_input)\n",
    "x2 = tf.keras.layers.GlobalMaxPooling1D()(x2)\n",
    "\n",
    "x = tf.keras.layers.concatenate([x1, x2])\n",
    "\n",
    "score_output = tf.keras.layers.Dense(1, name='score_output')(x)\n",
    "class_output = tf.keras.layers.Dense(5, name='class_output')(x)\n",
    "\n",
    "model = tf.keras.Model(inputs=[image_input, timeseries_input],\n",
    "                       outputs=[score_output, class_output])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "让我们绘制这个模型，以便您可以清楚地看到我们在这里做什么（请注意，图中显示的形状是批处理形状，而不是按样本的形状）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function plot_model in module tensorflow.python.keras.utils.vis_utils:\n",
      "\n",
      "plot_model(model, to_file='model.png', show_shapes=False, show_layer_names=True, rankdir='TB', expand_nested=False, dpi=96)\n",
      "    Converts a Keras model to dot format and save to a file.\n",
      "    \n",
      "    Arguments:\n",
      "      model: A Keras model instance\n",
      "      to_file: File name of the plot image.\n",
      "      show_shapes: whether to display shape information.\n",
      "      show_layer_names: whether to display layer names.\n",
      "      rankdir: `rankdir` argument passed to PyDot,\n",
      "          a string specifying the format of the plot:\n",
      "          'TB' creates a vertical plot;\n",
      "          'LR' creates a horizontal plot.\n",
      "      expand_nested: Whether to expand nested models into clusters.\n",
      "      dpi: Dots per inch.\n",
      "    \n",
      "    Returns:\n",
      "      A Jupyter notebook Image object if Jupyter is installed.\n",
      "      This enables in-line display of the model plots in notebooks.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tf.keras.utils.plot_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to import pydot. You must install pydot and graphviz for `pydotprint` to work.\n"
     ]
    }
   ],
   "source": [
    "# https://blog.csdn.net/weixin_42459037/article/details/84066164\n",
    "\n",
    "tf.keras.utils.plot_model(model, 'multi_input_and_output_model.png', show_shapes=True,dpi=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 损失函数\n",
    "在编译时，通过将损失函数作为列表传递，我们可以为不同的输出指定不同的损失："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.RMSprop(1e-3),\n",
    "    loss=[tf.keras.losses.MeanSquaredError(),\n",
    "          tf.keras.losses.CategoricalCrossentropy(from_logits=True)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果我们仅将单个损失函数传递给模型，则将相同的损失函数应用于每个输出，这在此处不合适。\n",
    "\n",
    "### 4.2 指标函数\n",
    "同样对于指标："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.RMSprop(1e-3),\n",
    "    loss=[tf.keras.losses.MeanSquaredError(),\n",
    "          tf.keras.losses.CategoricalCrossentropy(from_logits=True)],\n",
    "    metrics=[\n",
    "        [tf.keras.metrics.MeanAbsolutePercentageError(),\n",
    "         tf.keras.metrics.MeanAbsoluteError()],\n",
    "        \n",
    "        [tf.keras.metrics.CategoricalAccuracy()]\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由于我们为输出层命名，因此我们还可以通过dict指定每个输出的损失和指标："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.RMSprop(1e-3),\n",
    "    loss={'score_output': tf.keras.losses.MeanSquaredError(),\n",
    "          'class_output': tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "         },\n",
    "    metrics={'score_output': [tf.keras.metrics.MeanAbsolutePercentageError(),\n",
    "                              tf.keras.metrics.MeanAbsoluteError()],\n",
    "             \n",
    "             'class_output': [tf.keras.metrics.CategoricalAccuracy()]}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果您有两个以上的输出，我们建议使用显式名称和字典。\n",
    "\n",
    "可以使用以下参数对不同的特定于输出的损失赋予不同的权重（例如，在我们的示例中，我们可能希望通过将某类损失函数赋予更高的权重）\n",
    "\n",
    "loss_weights："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.RMSprop(1e-3),\n",
    "    loss={'score_output': tf.keras.losses.MeanSquaredError(),\n",
    "          'class_output': tf.keras.losses.CategoricalCrossentropy(from_logits=True)},\n",
    "    metrics={'score_output': [tf.keras.metrics.MeanAbsolutePercentageError(),\n",
    "                              tf.keras.metrics.MeanAbsoluteError()],\n",
    "             'class_output': [tf.keras.metrics.CategoricalAccuracy()]},\n",
    "    loss_weights={'score_output': 2., 'class_output': 1.})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "您还可以选择不为某些输出计算损失，如果这些输出仅用于预测而不是训练："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List loss version\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.RMSprop(1e-3),\n",
    "    loss=[None, tf.keras.losses.CategoricalCrossentropy(from_logits=True)])\n",
    "\n",
    "# Or dict loss version\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.RMSprop(1e-3),\n",
    "    loss={'class_output':tf.keras.losses.CategoricalCrossentropy(from_logits=True)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3完整运行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_input = tf.keras.Input(shape=(32, 32, 3), name='img_input')\n",
    "timeseries_input = tf.keras.Input(shape=(20, 10), name='ts_input')\n",
    "\n",
    "x1 = tf.keras.layers.Conv2D(3, 3)(image_input)\n",
    "x1 = tf.keras.layers.GlobalMaxPooling2D()(x1)\n",
    "\n",
    "\n",
    "x2 = tf.keras.layers.Conv1D(3, 3)(timeseries_input)\n",
    "x2 = tf.keras.layers.GlobalMaxPooling1D()(x2)\n",
    "\n",
    "x = tf.keras.layers.concatenate([x1, x2])\n",
    "\n",
    "score_output = tf.keras.layers.Dense(1, name='score_output')(x)\n",
    "class_output = tf.keras.layers.Dense(5, name='class_output')(x)\n",
    "\n",
    "model = tf.keras.Model(inputs=[image_input, timeseries_input],\n",
    "                    outputs=[score_output, class_output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 6.4241 - score_output_loss: 0.3581 - class_output_loss: 6.0660\n",
      "Epoch 2/3\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 5.8504 - score_output_loss: 0.2064 - class_output_loss: 5.6440\n",
      "Epoch 3/3\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 5.5286 - score_output_loss: 0.1412 - class_output_loss: 5.3874\n",
      "Epoch 1/3\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 5.2926 - score_output_loss: 0.1067 - class_output_loss: 5.1859\n",
      "Epoch 2/3\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 5.1511 - score_output_loss: 0.0917 - class_output_loss: 5.0594\n",
      "Epoch 3/3\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 5.0309 - score_output_loss: 0.0834 - class_output_loss: 4.9475\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1d990d4b518>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.RMSprop(1e-3),\n",
    "    loss=[tf.keras.losses.MeanSquaredError(),\n",
    "          tf.keras.losses.CategoricalCrossentropy(from_logits=True)])\n",
    "\n",
    "# Generate dummy Numpy data\n",
    "import numpy as np\n",
    "img_data = np.random.random_sample(size=(100, 32, 32, 3))\n",
    "ts_data = np.random.random_sample(size=(100, 20, 10))\n",
    "score_targets = np.random.random_sample(size=(100, 1))\n",
    "class_targets = np.random.random_sample(size=(100, 5))\n",
    "\n",
    "# Fit on lists\n",
    "model.fit([img_data, ts_data], [score_targets, class_targets],\n",
    "          batch_size=32,\n",
    "          epochs=3)\n",
    "\n",
    "# Alternatively, fit on dicts\n",
    "model.fit({'img_input': img_data, 'ts_input': ts_data},\n",
    "          {'score_output': score_targets, 'class_output': class_targets},\n",
    "          batch_size=32,\n",
    "          epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 5.2131 - score_output_loss: 0.0994 - class_output_loss: 5.1137\n",
      "Epoch 2/3\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 4.9949 - score_output_loss: 0.1076 - class_output_loss: 4.8873\n",
      "Epoch 3/3\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 5.0171 - score_output_loss: 0.1119 - class_output_loss: 4.9052\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1d990e24f28>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.RMSprop(1e-3),\n",
    "    loss=[tf.keras.losses.MeanSquaredError(),\n",
    "          tf.keras.losses.CategoricalCrossentropy(from_logits=True)])\n",
    "\n",
    "# Generate dummy Numpy data\n",
    "import numpy as np\n",
    "img_data = np.random.random_sample(size=(100, 32, 32, 3))\n",
    "ts_data = np.random.random_sample(size=(100, 20, 10))\n",
    "score_targets = np.random.random_sample(size=(100, 1))\n",
    "class_targets = np.random.random_sample(size=(100, 5))\n",
    "\n",
    "\n",
    "\n",
    "# Alternatively, fit on dicts\n",
    "model.fit((img_data,  ts_data),\n",
    "          (score_targets, class_targets),\n",
    "          batch_size=32,\n",
    "          epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
